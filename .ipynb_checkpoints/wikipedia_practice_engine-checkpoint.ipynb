{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia, nltk, numpy as np, pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_pos(raw_text, pos):\n",
    "    '''\n",
    "    This function will take in raw text and return a pandas DataFrame containing a unique set of words \n",
    "    whose parts of speech match what is defined\n",
    "    raw_text: raw string from data source\n",
    "    List pos: a list of strings that define what parts of speech is returned want\n",
    "    '''\n",
    "    columns = ['word', 'tag']\n",
    "    tagged = nltk.pos_tag(nltk.word_tokenize(raw_text))\n",
    "    df = pd.DataFrame([x for x in tagged], columns=columns)\n",
    "    filtered = df[df.tag.isin(pos)].drop_duplicates().reset_index(drop=True)\n",
    "    return filtered\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeSampleDataset(list_of_foods, pos):\n",
    "    dishes_row = []\n",
    "    dd_columns = ['title', 'description']\n",
    "    verified_list = []\n",
    "    \n",
    "    for i in range(len(list_of_foods)):\n",
    "        wikipedia_title = (wikipedia.WikipediaPage(title = list_of_foods[i]).title)\n",
    "        if wikipedia_title != None:\n",
    "            verified_list.append(list_of_foods[i])\n",
    "            \n",
    "    for i in range(len(verified_list)):\n",
    "        wikipedia_content = (wikipedia.WikipediaPage(title = list_of_foods[i]).content)\n",
    "        filtered_item = filter_pos(wikipedia_content, pos)\n",
    "        \n",
    "        description = np.array([])\n",
    "        for j in range(len(filtered_item)):\n",
    "            description = np.append(description, filtered_item.word[j])\n",
    "            \n",
    "        tup = (list_of_foods[i], ', '.join(description))\n",
    "        dishes_row.append(tup)\n",
    "        \n",
    "    dish_description_df = pd.DataFrame([x for x in dishes_row], columns = dd_columns)\n",
    "    return dish_description_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that takes in movie title as input and outputs most similar movies\n",
    "def get_recommendations(title, data):\n",
    "    \n",
    "    #Define a TF-IDF Vectorizer Object. Remove all english stop words such as 'the', 'a'\n",
    "    tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "    #Construct the required TF-IDF matrix by fitting and transforming the data\n",
    "    tfidf_matrix = tfidf.fit_transform(data['description'])\n",
    "\n",
    "    # Compute the cosine similarity matrix\n",
    "    cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "    \n",
    "    #Construct a reverse map of indices and movie titles\n",
    "    indices = pd.Series(data.index, index=data['title']).drop_duplicates()\n",
    "    \n",
    "    # Get the index of the movie that matches the title\n",
    "    idx = indices[title]\n",
    "\n",
    "    # Get the pairwsie similarity scores of all movies with that movie\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "\n",
    "    # Sort the movies based on the similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the scores of the 10 most similar movies\n",
    "    sim_scores = sim_scores[1:11]\n",
    "\n",
    "    # Get the movie indices\n",
    "    item_indices = [i[0] for i in sim_scores]\n",
    "\n",
    "    # Return the top 10 most similar movies\n",
    "    return data['title'].iloc[item_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = ['JJ', 'VB', 'VBG', 'VBP', 'VvBZ']\n",
    "pos_simple = ['JJ', 'NN']\n",
    "# sampleData = makeSampleDataset(['filet mignon', 'steak', 'spaghetti', 'macaroni', 'apple', 'banana', 'pear'], pos_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sampleData' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-12039d91f530>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msampleData\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sampleData' is not defined"
     ]
    }
   ],
   "source": [
    "sampleData;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_recommendations('macaroni', sampleData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# foodList = pd.DataFrame(pd.read_csv('foodlist.csv'))\n",
    "# foodList = foodList.loc[:, ~foodList.columns.str.contains('^Unnamed')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#foodSampleData = makeSampleDataset(foodList['items'], pos_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "foodSampleData = pd.read_pickle('foodSampleData')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4       Apple butter\n",
       "86         Hamburger\n",
       "170            Steak\n",
       "75     Fried chicken\n",
       "151      Ribs (food)\n",
       "21     Cajun cuisine\n",
       "7           Barbecue\n",
       "1              Bread\n",
       "127    Peanut butter\n",
       "129         Pemmican\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_recommendations('Jerky', foodSampleData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "foodSampleData.to_pickle('foodSampleData')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title                                                  Apple Pie\n",
      "description    apple, pie, tart, principal, ingredient, occas...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(foodSampleData.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named keras",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1b3008b4a380>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named keras"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/Users/jamesho/Desktop/Projects/Project-RR/jupyterNotebooks'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
